{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Graph Neural Networks Actually Work\n",
    "\n",
    "This notebook builds GNNs **from scratch** so you can see exactly what happens at every step. No black boxes.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Why Graphs? The Problem with Regular Neural Networks](#1)\n",
    "2. [Graphs as Data Structures](#2)\n",
    "3. [The Core Idea: Message Passing (by hand)](#3)\n",
    "4. [Building a GNN Layer from Scratch in PyTorch](#4)\n",
    "5. [Full GNN Model: Node Classification Example](#5)\n",
    "6. [Edge Features — What MeshGraphNets Adds](#6)\n",
    "7. [Summary: The GNN Mental Model](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='1'></a>\n",
    "# 1. Why Graphs? The Problem with Regular Neural Networks\n",
    "\n",
    "Regular neural networks assume structured input:\n",
    "\n",
    "| Network | Assumes input is... | Example |\n",
    "|---------|--------------------|---------|\n",
    "| MLP | Fixed-size vector | `[age, height, weight]` |\n",
    "| CNN | Regular grid | Images (pixels on a grid) |\n",
    "| RNN | Sequence | Text, time series |\n",
    "\n",
    "But many real-world data are **irregular**:\n",
    "- Social networks (users + friendships)\n",
    "- Molecules (atoms + bonds)\n",
    "- **Simulation meshes** (nodes + elements) ← this is why MeshGraphNets uses GNNs\n",
    "- Citation networks, road networks, power grids...\n",
    "\n",
    "You **can't** flatten a mesh into a grid without losing structure. GNNs operate **directly on the graph**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='2'></a>\n",
    "# 2. Graphs as Data Structures\n",
    "\n",
    "A graph is just: **nodes** (with features) + **edges** (connections between nodes).\n",
    "\n",
    "```\n",
    "    (0)───(1)\n",
    "     │   / │\n",
    "     │  /  │\n",
    "     │ /   │\n",
    "    (2)───(3)\n",
    "```\n",
    "\n",
    "Let's build this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 4, each with 3 features\n",
      "Edges: 10 (directed, so 5 undirected edges × 2)\n",
      "\n",
      "Node features:\n",
      "[[1.  0.  0.5]\n",
      " [0.  1.  0.3]\n",
      " [0.5 0.5 0.8]\n",
      " [0.2 0.8 0.1]]\n",
      "\n",
      "Edge list (src → dst):\n",
      "  0 → 1\n",
      "  1 → 0\n",
      "  0 → 2\n",
      "  2 → 0\n",
      "  1 → 2\n",
      "  2 → 1\n",
      "  1 → 3\n",
      "  3 → 1\n",
      "  2 → 3\n",
      "  3 → 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# =============================================\n",
    "# STEP 1: Define the graph structure\n",
    "# =============================================\n",
    "\n",
    "# Node features: each node has a 3-dimensional feature vector\n",
    "# Think of this as: [temperature, pressure, velocity] at each mesh node\n",
    "node_features = np.array([\n",
    "    [1.0, 0.0, 0.5],   # Node 0\n",
    "    [0.0, 1.0, 0.3],   # Node 1\n",
    "    [0.5, 0.5, 0.8],   # Node 2\n",
    "    [0.2, 0.8, 0.1],   # Node 3\n",
    "])\n",
    "\n",
    "# Edge list: pairs of (source, target)\n",
    "# Undirected edges → we list both directions\n",
    "edges = [\n",
    "    (0, 1), (1, 0),  # 0 ↔ 1\n",
    "    (0, 2), (2, 0),  # 0 ↔ 2\n",
    "    (1, 2), (2, 1),  # 1 ↔ 2\n",
    "    (1, 3), (3, 1),  # 1 ↔ 3\n",
    "    (2, 3), (3, 2),  # 2 ↔ 3\n",
    "]\n",
    "\n",
    "# Separate into source and target arrays (common format)\n",
    "src = [e[0] for e in edges]\n",
    "dst = [e[1] for e in edges]\n",
    "\n",
    "print(f\"Nodes: {len(node_features)}, each with {node_features.shape[1]} features\")\n",
    "print(f\"Edges: {len(edges)} (directed, so 5 undirected edges × 2)\")\n",
    "print(f\"\\nNode features:\\n{node_features}\")\n",
    "print(f\"\\nEdge list (src → dst):\")\n",
    "for s, d in edges:\n",
    "    print(f\"  {s} → {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix A:\n",
      "[[0. 1. 1. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]]\n",
      "\n",
      "Node 0's neighbors: [1, 2]\n",
      "Node 1's neighbors: [0, 2, 3]\n",
      "Node 2's neighbors: [0, 1, 3]\n",
      "Node 3's neighbors: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# ADJACENCY MATRIX: another way to represent edges\n",
    "# =============================================\n",
    "\n",
    "# A[i][j] = 1 if there's an edge from i to j\n",
    "num_nodes = len(node_features)\n",
    "A = np.zeros((num_nodes, num_nodes))\n",
    "for s, d in edges:\n",
    "    A[s][d] = 1\n",
    "\n",
    "print(\"Adjacency matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Each row tells you who a node's neighbors are:\n",
    "for i in range(num_nodes):\n",
    "    neighbors = np.where(A[i] == 1)[0]\n",
    "    print(f\"Node {i}'s neighbors: {neighbors.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='3'></a>\n",
    "# 3. The Core Idea: Message Passing (by hand)\n",
    "\n",
    "**This is the entire idea behind GNNs.** Everything else is details.\n",
    "\n",
    "Each GNN layer does three things:\n",
    "\n",
    "```\n",
    "For each node i:\n",
    "  1. MESSAGE:    Collect features from all neighbors j\n",
    "  2. AGGREGATE:  Combine those messages (sum, mean, max)\n",
    "  3. UPDATE:     Compute new feature for node i using its own feature + aggregated messages\n",
    "```\n",
    "\n",
    "That's it. Let's do it **manually with numpy** first — no PyTorch, no libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP-BY-STEP MESSAGE PASSING (1 layer, no learning yet)\n",
      "============================================================\n",
      "\n",
      "Original node features X:\n",
      "  Node 0: [1.  0.  0.5]\n",
      "  Node 1: [0.  1.  0.3]\n",
      "  Node 2: [0.5 0.5 0.8]\n",
      "  Node 3: [0.2 0.8 0.1]\n",
      "\n",
      "--- Step 1: MESSAGE (gather neighbor features) ---\n",
      "  Node 0 receives messages from neighbors [1, 2]:\n",
      "    message from 1: [0.  1.  0.3]\n",
      "    message from 2: [0.5 0.5 0.8]\n",
      "  Node 1 receives messages from neighbors [0, 2, 3]:\n",
      "    message from 0: [1.  0.  0.5]\n",
      "    message from 2: [0.5 0.5 0.8]\n",
      "    message from 3: [0.2 0.8 0.1]\n",
      "  Node 2 receives messages from neighbors [0, 1, 3]:\n",
      "    message from 0: [1.  0.  0.5]\n",
      "    message from 1: [0.  1.  0.3]\n",
      "    message from 3: [0.2 0.8 0.1]\n",
      "  Node 3 receives messages from neighbors [1, 2]:\n",
      "    message from 1: [0.  1.  0.3]\n",
      "    message from 2: [0.5 0.5 0.8]\n",
      "\n",
      "--- Step 2: AGGREGATE (sum of neighbor features) ---\n",
      "  This is just A @ X (matrix multiply!)\n",
      "  Node 0: sum of neighbors [1, 2] = [0.5 1.5 1.1]\n",
      "  Node 1: sum of neighbors [0, 2, 3] = [1.7 1.3 1.4]\n",
      "  Node 2: sum of neighbors [0, 1, 3] = [1.2 1.8 0.9]\n",
      "  Node 3: sum of neighbors [1, 2] = [0.5 1.5 1.1]\n",
      "\n",
      "--- Step 3: UPDATE (own features + aggregated) ---\n",
      "  Node 0: [1.  0.  0.5] + [0.5 1.5 1.1] = [1.5 1.5 1.6]\n",
      "  Node 1: [0.  1.  0.3] + [1.7 1.3 1.4] = [1.7 2.3 1.7]\n",
      "  Node 2: [0.5 0.5 0.8] + [1.2 1.8 0.9] = [1.7 2.3 1.7]\n",
      "  Node 3: [0.2 0.8 0.1] + [0.5 1.5 1.1] = [0.7 2.3 1.2]\n",
      "\n",
      "============================================================\n",
      "After 1 round of message passing, each node's features\n",
      "now contain information from its IMMEDIATE neighbors.\n",
      "After 2 rounds → 2-hop neighborhood.\n",
      "After k rounds → k-hop neighborhood.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# MESSAGE PASSING BY HAND (pure numpy)\n",
    "# =============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP-BY-STEP MESSAGE PASSING (1 layer, no learning yet)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X = node_features.copy()  # Shape: (4 nodes, 3 features)\n",
    "\n",
    "print(f\"\\nOriginal node features X:\")\n",
    "for i in range(num_nodes):\n",
    "    print(f\"  Node {i}: {X[i]}\")\n",
    "\n",
    "# ----- STEP 1: MESSAGE -----\n",
    "# For each node, gather the features of its neighbors\n",
    "print(f\"\\n--- Step 1: MESSAGE (gather neighbor features) ---\")\n",
    "for i in range(num_nodes):\n",
    "    neighbors = np.where(A[i] == 1)[0]\n",
    "    print(f\"  Node {i} receives messages from neighbors {neighbors.tolist()}:\")\n",
    "    for j in neighbors:\n",
    "        print(f\"    message from {j}: {X[j]}\")\n",
    "\n",
    "# ----- STEP 2: AGGREGATE (sum) -----\n",
    "# Sum up all neighbor features for each node\n",
    "# This is literally just matrix multiplication: A @ X\n",
    "aggregated = A @ X\n",
    "\n",
    "print(f\"\\n--- Step 2: AGGREGATE (sum of neighbor features) ---\")\n",
    "print(f\"  This is just A @ X (matrix multiply!)\")\n",
    "for i in range(num_nodes):\n",
    "    neighbors = np.where(A[i] == 1)[0]\n",
    "    manual_sum = sum(X[j] for j in neighbors)\n",
    "    print(f\"  Node {i}: sum of neighbors {neighbors.tolist()} = {aggregated[i]}\")\n",
    "\n",
    "# ----- STEP 3: UPDATE -----\n",
    "# Combine own features with aggregated neighbor features\n",
    "# Simplest version: just add them\n",
    "X_new = X + aggregated\n",
    "\n",
    "print(f\"\\n--- Step 3: UPDATE (own features + aggregated) ---\")\n",
    "for i in range(num_nodes):\n",
    "    print(f\"  Node {i}: {X[i]} + {aggregated[i]} = {X_new[i]}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"After 1 round of message passing, each node's features\")\n",
    "print(f\"now contain information from its IMMEDIATE neighbors.\")\n",
    "print(f\"After 2 rounds → 2-hop neighborhood.\")\n",
    "print(f\"After k rounds → k-hop neighborhood.\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# WHY 15 ROUNDS IN MESHGRAPHNETS?\n",
    "# =============================================\n",
    "\n",
    "# Let's see how information propagates through the graph.\n",
    "# After k rounds, node i has information from all nodes within k hops.\n",
    "\n",
    "# A^k tells you how many k-hop paths exist between nodes\n",
    "print(\"Reachability after k message-passing rounds:\")\n",
    "print(\"(non-zero means information can flow between those nodes)\\n\")\n",
    "\n",
    "A_power = np.eye(num_nodes)  # A^0 = identity (each node knows itself)\n",
    "for k in range(1, 4):\n",
    "    A_power = A_power @ A\n",
    "    reachable = (A_power > 0).astype(int)\n",
    "    print(f\"After {k} round(s):\")\n",
    "    print(reachable)\n",
    "    \n",
    "    # Check if fully connected\n",
    "    if reachable.all():\n",
    "        print(f\"→ ALL nodes can reach ALL other nodes after {k} rounds!\\n\")\n",
    "        break\n",
    "    print()\n",
    "\n",
    "print(\"MeshGraphNets uses 15 rounds because simulation meshes are\")\n",
    "print(\"much larger — information needs to travel across the entire mesh.\")\n",
    "print(\"15 hops ≈ the diameter of typical simulation meshes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='4'></a>\n",
    "# 4. Building a GNN Layer from Scratch in PyTorch\n",
    "\n",
    "Now let's add **learnable parameters**. The simplest GNN layer (like GCN) does:\n",
    "\n",
    "$$h_i^{(l+1)} = \\sigma\\left( W \\cdot \\text{AGGREGATE}\\left(\\{h_j^{(l)} : j \\in \\mathcal{N}(i) \\cup \\{i\\}\\}\\right) \\right)$$\n",
    "\n",
    "In plain English:\n",
    "1. Gather neighbor features (including self)\n",
    "2. Aggregate (sum or mean)\n",
    "3. Multiply by a learnable weight matrix W\n",
    "4. Apply activation function (ReLU)\n",
    "\n",
    "That's a full GNN layer. Let's build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# GNN LAYER FROM SCRATCH\n",
    "# =============================================\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    \"\"\"One layer of a basic Graph Neural Network.\n",
    "    \n",
    "    Does: h_i' = ReLU( W · mean({h_j : j ∈ neighbors(i) ∪ {i}}) )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # This is the ONLY learnable parameter — a linear transformation\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, X, edge_index):\n",
    "        \"\"\"\n",
    "        X:          (num_nodes, in_features)  — node feature matrix\n",
    "        edge_index: (2, num_edges)            — [src_nodes; dst_nodes]\n",
    "        \"\"\"\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        num_nodes = X.shape[0]\n",
    "        \n",
    "        # ----- STEP 1: MESSAGE -----\n",
    "        # Gather source node features for each edge\n",
    "        messages = X[src]  # Shape: (num_edges, in_features)\n",
    "        # messages[k] = feature vector of the SOURCE node of edge k\n",
    "        \n",
    "        # ----- STEP 2: AGGREGATE (mean) -----\n",
    "        # For each destination node, average all incoming messages\n",
    "        # Using scatter_mean: group messages by destination node, then average\n",
    "        agg = torch.zeros(num_nodes, X.shape[1])\n",
    "        count = torch.zeros(num_nodes, 1)\n",
    "        \n",
    "        # Accumulate messages at destination nodes\n",
    "        for k in range(len(src)):\n",
    "            agg[dst[k]] += messages[k]\n",
    "            count[dst[k]] += 1\n",
    "        \n",
    "        # Add self-loop (each node includes its OWN features)\n",
    "        agg += X\n",
    "        count += 1\n",
    "        \n",
    "        # Mean aggregation\n",
    "        agg = agg / count\n",
    "        \n",
    "        # ----- STEP 3: UPDATE -----\n",
    "        # Linear transformation + activation\n",
    "        out = self.linear(agg)  # W @ agg + b\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# ---- Test it ----\n",
    "X = torch.tensor(node_features, dtype=torch.float32)\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "layer = GNNLayer(in_features=3, out_features=4)  # 3 input features → 4 output features\n",
    "\n",
    "print(f\"Input shape:  {X.shape}  (4 nodes, 3 features each)\")\n",
    "output = layer(X, edge_index)\n",
    "print(f\"Output shape: {output.shape}  (4 nodes, 4 features each)\")\n",
    "print(f\"\\nOutput (each node now has a 4-dim learned representation):\")\n",
    "for i in range(num_nodes):\n",
    "    print(f\"  Node {i}: {output[i].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# VECTORIZED VERSION (how it's actually done)\n",
    "# =============================================\n",
    "# The loop above is slow. In practice, we use scatter operations.\n",
    "# Here's the clean version:\n",
    "\n",
    "class GNNLayerFast(nn.Module):\n",
    "    \"\"\"Same as above but vectorized using scatter_add.\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, X, edge_index):\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        num_nodes = X.shape[0]\n",
    "        \n",
    "        # Message: gather source features\n",
    "        messages = X[src]  # (num_edges, features)\n",
    "        \n",
    "        # Aggregate: scatter_add groups messages by destination\n",
    "        agg = torch.zeros_like(X)\n",
    "        agg.scatter_add_(0, dst.unsqueeze(1).expand_as(messages), messages)\n",
    "        \n",
    "        # Add self-loop + compute mean\n",
    "        degree = torch.zeros(num_nodes)\n",
    "        degree.scatter_add_(0, dst, torch.ones(len(dst)))\n",
    "        degree = degree + 1  # +1 for self-loop\n",
    "        agg = (agg + X) / degree.unsqueeze(1)\n",
    "        \n",
    "        # Update\n",
    "        return F.relu(self.linear(agg))\n",
    "\n",
    "\n",
    "# Verify both produce same results (with same weights)\n",
    "layer_fast = GNNLayerFast(3, 4)\n",
    "layer_fast.linear.weight = layer.linear.weight\n",
    "layer_fast.linear.bias = layer.linear.bias\n",
    "\n",
    "output_fast = layer_fast(X, edge_index)\n",
    "print(f\"Outputs match: {torch.allclose(output, output_fast, atol=1e-6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize what just happened\n",
    "\n",
    "```\n",
    "BEFORE (raw features):              AFTER 1 GNN layer:\n",
    "\n",
    "Node 0: [temp=1.0, pres=0.0, vel=0.5]    Node 0: [?, ?, ?, ?]  ← now 4-dim,\n",
    "Node 1: [temp=0.0, pres=1.0, vel=0.3]    Node 1: [?, ?, ?, ?]     encodes info\n",
    "Node 2: [temp=0.5, pres=0.5, vel=0.8]    Node 2: [?, ?, ?, ?]     from neighbors\n",
    "Node 3: [temp=0.2, pres=0.8, vel=0.1]    Node 3: [?, ?, ?, ?]\n",
    "\n",
    "What happened to Node 0:\n",
    "  1. Gathered features from neighbors {1, 2} and self {0}\n",
    "  2. Averaged them: mean([1,0,.5], [0,1,.3], [.5,.5,.8]) = [0.5, 0.5, 0.53]\n",
    "  3. Applied W @ [0.5, 0.5, 0.53] + b, then ReLU\n",
    "  4. Node 0 now \"knows about\" the temperature/pressure/velocity of its neighbors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='5'></a>\n",
    "# 5. Full GNN Model: Node Classification Example\n",
    "\n",
    "Let's build a complete GNN that actually **trains** on a task. We'll create a small graph where node color (label) depends on neighborhood structure, and train the GNN to predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# CREATE A SMALL DATASET\n",
    "# =============================================\n",
    "# \n",
    "# Graph: two clusters connected by a bridge\n",
    "#\n",
    "#   Cluster A (label=0)       Cluster B (label=1)\n",
    "#     0 ── 1                    4 ── 5\n",
    "#     │  ╲ │                    │  ╲ │\n",
    "#     2 ── 3 ──── bridge ──── 6 ── 7\n",
    "#\n",
    "# Task: predict which cluster each node belongs to\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Node features: random (the GNN must learn from STRUCTURE, not features)\n",
    "X_train = torch.randn(8, 3)\n",
    "\n",
    "# Labels: 0 for cluster A (nodes 0-3), 1 for cluster B (nodes 4-7)\n",
    "y_train = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "# Edges (undirected → list both directions)\n",
    "edge_pairs = [\n",
    "    # Cluster A (dense connections)\n",
    "    (0,1),(1,0), (0,2),(2,0), (0,3),(3,0), (1,3),(3,1), (2,3),(3,2),\n",
    "    # Bridge\n",
    "    (3,6),(6,3),\n",
    "    # Cluster B (dense connections)\n",
    "    (4,5),(5,4), (4,6),(6,4), (4,7),(7,4), (5,7),(7,5), (6,7),(7,6),\n",
    "]\n",
    "edge_index_train = torch.tensor([[e[0] for e in edge_pairs],\n",
    "                                  [e[1] for e in edge_pairs]])\n",
    "\n",
    "print(f\"Graph: {X_train.shape[0]} nodes, {len(edge_pairs)} directed edges\")\n",
    "print(f\"Labels: {y_train.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# FULL GNN MODEL (2 layers + classifier)\n",
    "# =============================================\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = GNNLayerFast(in_features, hidden_dim)  # Our custom layer!\n",
    "        self.layer2 = GNNLayerFast(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, X, edge_index):\n",
    "        # Layer 1: each node learns about 1-hop neighbors\n",
    "        h = self.layer1(X, edge_index)  # (N, hidden_dim)\n",
    "        \n",
    "        # Layer 2: each node now learns about 2-hop neighbors\n",
    "        h = self.layer2(h, edge_index)  # (N, hidden_dim)\n",
    "        \n",
    "        # Classify each node\n",
    "        out = self.classifier(h)  # (N, num_classes)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = GNN(in_features=3, hidden_dim=16, num_classes=2)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# TRAIN THE GNN\n",
    "# =============================================\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: GNN predicts label for EVERY node at once\n",
    "    logits = model(X_train, edge_index_train)  # (8, 2)\n",
    "    \n",
    "    # Loss: compare predictions to ground truth\n",
    "    loss = loss_fn(logits, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0 or epoch == 199:\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y_train).float().mean()\n",
    "        print(f\"  Epoch {epoch:3d}: loss={loss.item():.4f}, acc={acc:.2f}\")\n",
    "\n",
    "# Final predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_train, edge_index_train).argmax(dim=1)\n",
    "    \n",
    "print(f\"\\nFinal predictions: {preds.tolist()}\")\n",
    "print(f\"Ground truth:      {y_train.tolist()}\")\n",
    "print(f\"Correct:           {['✓' if p==y else '✗' for p,y in zip(preds, y_train)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why this works\n",
    "\n",
    "The GNN learned to classify nodes by their **neighborhood structure**:\n",
    "- Nodes in cluster A are densely connected to each other\n",
    "- Nodes in cluster B are densely connected to each other\n",
    "- Only 1 edge bridges the two clusters\n",
    "\n",
    "After 2 message-passing layers, each node has aggregated features from its 2-hop neighborhood — which captures enough cluster structure to classify correctly.\n",
    "\n",
    "**This is exactly what MeshGraphNets does**, but instead of predicting a class label, it predicts the **next-step physics** (acceleration, velocity change) at each mesh node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='6'></a>\n",
    "# 6. Edge Features — What MeshGraphNets Adds\n",
    "\n",
    "Our basic GNN only uses **node features**. But in MeshGraphNets, **edges also have features** (relative position, distance, etc.). This is crucial:\n",
    "\n",
    "| Basic GNN | MeshGraphNets |\n",
    "|-----------|---------------|\n",
    "| Message = neighbor's node features | Message = f(sender features, receiver features, **edge features**) |\n",
    "| Edge just means \"connected\" | Edge carries **geometric information** (relative displacement, distance) |\n",
    "\n",
    "Let's build a GNN layer with edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# GNN LAYER WITH EDGE FEATURES\n",
    "# (This is what MeshGraphNets actually uses)\n",
    "# =============================================\n",
    "\n",
    "class EdgeGNNLayer(nn.Module):\n",
    "    \"\"\"GNN layer that uses edge features in message computation.\n",
    "    \n",
    "    Message:   m_ij = MLP_edge([h_i; h_j; e_ij])\n",
    "    Aggregate: agg_i = sum(m_ij for j in neighbors)\n",
    "    Update:    h_i' = MLP_node([h_i; agg_i])\n",
    "    \n",
    "    This is the architecture used in MeshGraphNets!\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features, edge_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Edge MLP: takes [src_node; dst_node; edge_feat] → message\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(node_features * 2 + edge_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # Node MLP: takes [node_feat; aggregated_messages] → updated node\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_features + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # LayerNorm (used in MeshGraphNets)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, X, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        X:         (num_nodes, node_features)\n",
    "        edge_index: (2, num_edges)\n",
    "        edge_attr:  (num_edges, edge_features) — e.g., relative position, distance\n",
    "        \"\"\"\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        num_nodes = X.shape[0]\n",
    "        \n",
    "        # ----- MESSAGE -----\n",
    "        # Concatenate: [sender_features, receiver_features, edge_features]\n",
    "        edge_input = torch.cat([X[src], X[dst], edge_attr], dim=1)\n",
    "        messages = self.edge_mlp(edge_input)  # (num_edges, hidden_dim)\n",
    "        \n",
    "        # ----- AGGREGATE (sum) -----\n",
    "        agg = torch.zeros(num_nodes, messages.shape[1])\n",
    "        agg.scatter_add_(0, dst.unsqueeze(1).expand_as(messages), messages)\n",
    "        \n",
    "        # ----- UPDATE -----\n",
    "        node_input = torch.cat([X, agg], dim=1)  # [own features; aggregated]\n",
    "        out = self.node_mlp(node_input)\n",
    "        \n",
    "        # Residual connection + LayerNorm (MeshGraphNets style)\n",
    "        # out = self.norm(X + out)  # Would need matching dims in practice\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "print(\"EdgeGNNLayer — the building block of MeshGraphNets\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DEMO: Mesh with spatial edge features\n",
    "# =============================================\n",
    "\n",
    "# Imagine a 2D mesh with 4 nodes at these positions:\n",
    "#   Node 0: (0, 0)     Node 1: (1, 0)\n",
    "#   Node 2: (0, 1)     Node 3: (1, 1)\n",
    "\n",
    "positions = torch.tensor([\n",
    "    [0.0, 0.0],  # Node 0\n",
    "    [1.0, 0.0],  # Node 1\n",
    "    [0.0, 1.0],  # Node 2\n",
    "    [1.0, 1.0],  # Node 3\n",
    "])\n",
    "\n",
    "# Node features: e.g., temperature at each node\n",
    "X_mesh = torch.tensor([\n",
    "    [100.0],  # Node 0: hot\n",
    "    [50.0],   # Node 1: warm\n",
    "    [50.0],   # Node 2: warm\n",
    "    [0.0],    # Node 3: cold\n",
    "])\n",
    "\n",
    "# Edges (same as before)\n",
    "edge_index_mesh = torch.tensor([src, dst])\n",
    "\n",
    "# ===== THE KEY PART: Edge features =====\n",
    "# For each edge (i→j), compute RELATIVE displacement and distance\n",
    "# This is exactly what MeshGraphNets does!\n",
    "\n",
    "src_pos = positions[edge_index_mesh[0]]  # (num_edges, 2)\n",
    "dst_pos = positions[edge_index_mesh[1]]  # (num_edges, 2)\n",
    "\n",
    "relative_disp = dst_pos - src_pos           # x_j - x_i  (relative position)\n",
    "distance = torch.norm(relative_disp, dim=1, keepdim=True)  # |x_j - x_i|\n",
    "\n",
    "# Edge features = [relative_displacement, distance]\n",
    "edge_attr_mesh = torch.cat([relative_disp, distance], dim=1)  # (num_edges, 3)\n",
    "\n",
    "print(\"Edge features (what MeshGraphNets uses):\")\n",
    "print(f\"{'Edge':<10} {'Rel. Disp (x,y)':<25} {'Distance':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for k, (s, d) in enumerate(edge_pairs[:len(src)]):\n",
    "    print(f\"{s} → {d:<6} {str(edge_attr_mesh[k][:2].tolist()):<25} {edge_attr_mesh[k][2].item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# RUN THE EDGE-AWARE GNN LAYER\n",
    "# =============================================\n",
    "\n",
    "edge_layer = EdgeGNNLayer(\n",
    "    node_features=1,   # Just temperature\n",
    "    edge_features=3,   # [rel_x, rel_y, distance]\n",
    "    hidden_dim=8\n",
    ")\n",
    "\n",
    "out = edge_layer(X_mesh, edge_index_mesh, edge_attr_mesh)\n",
    "\n",
    "print(f\"Input:  {X_mesh.shape}  — 4 nodes, 1 feature (temperature)\")\n",
    "print(f\"Output: {out.shape}  — 4 nodes, 8 features (learned representation)\")\n",
    "print(f\"\\nEach node's representation now encodes:\")\n",
    "print(f\"  - Its own temperature\")\n",
    "print(f\"  - Neighbors' temperatures\")\n",
    "print(f\"  - WHERE those neighbors are (via edge features)\")\n",
    "print(f\"  - HOW FAR they are (via distance)\")\n",
    "print(f\"\\nThis is why MeshGraphNets can learn physics — the edge features\")\n",
    "print(f\"encode the spatial relationships that govern physical interactions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='7'></a>\n",
    "# 7. Summary: The GNN Mental Model\n",
    "\n",
    "## The 3-Step Recipe (memorize this)\n",
    "\n",
    "```\n",
    "Every GNN layer does:\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                                                                 │\n",
    "│  1. MESSAGE:    For each edge (i,j), compute a message          │\n",
    "│                 m_ij = f(h_i, h_j, e_ij)                       │\n",
    "│                         ↑     ↑     ↑                           │\n",
    "│                       sender receiver edge                      │\n",
    "│                       node   node    features                   │\n",
    "│                                                                 │\n",
    "│  2. AGGREGATE:  For each node i, combine incoming messages      │\n",
    "│                 agg_i = Σ m_ij   (or mean, or max)              │\n",
    "│                         j∈N(i)                                  │\n",
    "│                                                                 │\n",
    "│  3. UPDATE:     Compute new node feature                        │\n",
    "│                 h_i' = g(h_i, agg_i)                            │\n",
    "│                         ↑      ↑                                │\n",
    "│                       old    aggregated                         │\n",
    "│                       self   neighbor info                      │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## How GNN Variants Differ\n",
    "\n",
    "| Variant | Message function | Aggregation | Key difference |\n",
    "|---------|-----------------|-------------|----------------|\n",
    "| **GCN** | W · h_j (just transform neighbor) | Normalized sum | Simplest |\n",
    "| **GAT** | α_ij · W · h_j (attention-weighted) | Weighted sum | Learns which neighbors matter more |\n",
    "| **GraphSAGE** | W · h_j | Mean, Max, or LSTM | Supports sampling for large graphs |\n",
    "| **MPNN** | MLP([h_i; h_j; e_ij]) | Sum | Uses edge features (general) |\n",
    "| **MeshGraphNets** | MLP([h_i; h_j; e_ij]) + residual + LayerNorm | Sum | MPNN + dual-space edges + noise training |\n",
    "\n",
    "## From Basic GNN → MeshGraphNets\n",
    "\n",
    "```\n",
    "Basic GNN (what we built):\n",
    "  ✓ Node features\n",
    "  ✓ Message passing\n",
    "  ✓ Learnable weights\n",
    "\n",
    "MeshGraphNets adds:\n",
    "  + Edge features (relative displacement, distance)\n",
    "  + Two edge types (mesh-space + world-space)\n",
    "  + 15 message-passing rounds (not 2)\n",
    "  + Residual connections + LayerNorm\n",
    "  + Predicts derivatives (not states)\n",
    "  + Noise injection during training\n",
    "  + Separate encoder/processor/decoder\n",
    "```\n",
    "\n",
    "## Key Intuitions\n",
    "\n",
    "1. **GNN layers ≈ diffusion**: Each layer spreads information 1 hop. After L layers, each node knows about its L-hop neighborhood.\n",
    "\n",
    "2. **Why it works for physics**: Physical interactions are LOCAL — a node's next-state depends on its nearby neighbors. This is exactly what message passing computes.\n",
    "\n",
    "3. **Why RELATIVE edge features**: If you use `x_j - x_i` instead of absolute positions, the model learns \"how neighbors interact\" — which is the same everywhere in space. A spring behaves the same whether it's at coordinates (0,0) or (100,100).\n",
    "\n",
    "4. **Why SUM not MEAN for physics**: Sum preserves the signal that \"this node has 10 neighbors pushing it\" vs \"1 neighbor pushing it.\" Mean would lose that distinction. Physical forces ADD up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
